# StudySpark Robots.txt
# This file tells search engines what to crawl and what to skip.

User-agent: *
# Block private/user-only sections
Disallow: /dashboard
Disallow: /upload
Disallow: /account
Disallow: /settings
Disallow: /api

# Allow everything else
Disallow:

# Point search engines to sitemap
Sitemap: https://studyspark.com/sitemap.xml
